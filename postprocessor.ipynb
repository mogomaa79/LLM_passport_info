{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import json\n",
    "import os\n",
    "import traceback\n",
    "\n",
    "# Import postprocessing functions\n",
    "from src.utils.passport_processing import postprocess\n",
    "from src.utils.results_utils import ResultsAgent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "dictionary update sequence element #0 has length 1; 2 is required",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mValueError\u001b[39m                                Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[3]\u001b[39m\u001b[32m, line 23\u001b[39m\n\u001b[32m      1\u001b[39m data = {\n\u001b[32m      2\u001b[39m  \u001b[33m\"\u001b[39m\u001b[33mnumber\u001b[39m\u001b[33m\"\u001b[39m: \u001b[33m\"\u001b[39m\u001b[33mP3704125B\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m      3\u001b[39m  \u001b[33m\"\u001b[39m\u001b[33mcountry\u001b[39m\u001b[33m\"\u001b[39m: \u001b[33m\"\u001b[39m\u001b[33mPHL\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m   (...)\u001b[39m\u001b[32m     18\u001b[39m  \u001b[33m\"\u001b[39m\u001b[33mmrzLine2\u001b[39m\u001b[33m\"\u001b[39m: \u001b[33m\"\u001b[39m\u001b[33mP3704125B4PHL9206274F2910297<<<<<<<<<<<<<<<08\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m     19\u001b[39m }\n\u001b[32m     21\u001b[39m json_data = json.dumps(data)\n\u001b[32m---> \u001b[39m\u001b[32m23\u001b[39m out = \u001b[43mpostprocess\u001b[49m\u001b[43m(\u001b[49m\u001b[43mjson_data\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     25\u001b[39m \u001b[38;5;28mprint\u001b[39m(out)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Downloads/Maids/LLM_passport_info/src/utils/passport_processing.py:11\u001b[39m, in \u001b[36mpostprocess\u001b[39m\u001b[34m(json_data)\u001b[39m\n\u001b[32m     10\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mpostprocess\u001b[39m(json_data):\n\u001b[32m---> \u001b[39m\u001b[32m11\u001b[39m     formatted_data = \u001b[38;5;28;43mdict\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mjson_data\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     12\u001b[39m     country = formatted_data[\u001b[33m\"\u001b[39m\u001b[33mcountry\u001b[39m\u001b[33m\"\u001b[39m]\n\u001b[32m     14\u001b[39m     string_fields = [\n\u001b[32m     15\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mnumber\u001b[39m\u001b[33m\"\u001b[39m, \u001b[33m\"\u001b[39m\u001b[33mcountry\u001b[39m\u001b[33m\"\u001b[39m, \u001b[33m\"\u001b[39m\u001b[33mname\u001b[39m\u001b[33m\"\u001b[39m, \u001b[33m\"\u001b[39m\u001b[33msurname\u001b[39m\u001b[33m\"\u001b[39m, \u001b[33m\"\u001b[39m\u001b[33mmiddle name\u001b[39m\u001b[33m\"\u001b[39m, \u001b[33m\"\u001b[39m\u001b[33mgender\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m     16\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mplace of birth\u001b[39m\u001b[33m\"\u001b[39m, \u001b[33m\"\u001b[39m\u001b[33mmother name\u001b[39m\u001b[33m\"\u001b[39m, \u001b[33m\"\u001b[39m\u001b[33mfather name\u001b[39m\u001b[33m\"\u001b[39m, \u001b[33m\"\u001b[39m\u001b[33mplace of issue\u001b[39m\u001b[33m\"\u001b[39m, \u001b[33m\"\u001b[39m\u001b[33mcountry of issue\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m     17\u001b[39m     ]\n",
      "\u001b[31mValueError\u001b[39m: dictionary update sequence element #0 has length 1; 2 is required"
     ]
    }
   ],
   "source": [
    "data = {\n",
    " \"number\": \"P3704125B\",\n",
    " \"country\": \"PHL\",\n",
    " \"name\": \"JUDITH RICEZA\",\n",
    " \"surname\": \"GASPAR\",\n",
    " \"middle name\": \"\",\n",
    " \"gender\": \"F\",\n",
    " \"place of birth\": \"SINILOAN LAGUNA\",\n",
    " \"birth date\": \"27 JUN 1992\",\n",
    " \"issue date\": \"30 OCT 2019\",\n",
    " \"expiry date\": \"29 OCT 2029\",\n",
    " \"mother name\": \"\",\n",
    " \"father name\": \"\",\n",
    " \"spouse name\": \"\",\n",
    " \"place of issue\": \"DFA MANILA\",\n",
    " \"country of issue\": \"PHILIPPINES\",\n",
    " \"mrzLine1\": \"P<PHLGASPAR<<JUDITH<RICEZA<<<<<<<<<<<<<<<<<<<\",\n",
    " \"mrzLine2\": \"P3704125B4PHL9206274F2910297<<<<<<<<<<<<<<<08\"\n",
    "}\n",
    "\n",
    "json_data = json.dumps(data)\n",
    "\n",
    "out = postprocess(json_data)\n",
    "\n",
    "print(out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_file(file_path):\n",
    "    project_name = os.path.basename(file_path).replace('_results.csv', '')\n",
    "    country = project_name.split(' - ')[0].strip()\n",
    "    print(f\"Detected country/dataset: {country}\")\n",
    "    \n",
    "    # Read the CSV file\n",
    "    df = pd.read_csv(file_path)\n",
    "    print(f\"Read {len(df)} rows from {os.path.basename(file_path)}\")\n",
    "    \n",
    "    # Apply postprocessing to each row\n",
    "    processed_rows = []\n",
    "    \n",
    "    for i, (_, row) in enumerate(df.iterrows()):\n",
    "        try:\n",
    "            # Extract output data\n",
    "            if 'output' in row:\n",
    "                output_dict = json.loads(row['output'])\n",
    "            else:\n",
    "                # Create output dict from output.* columns\n",
    "                output_dict = {}\n",
    "                for col in row.index:\n",
    "                    if col.startswith('outputs.'):\n",
    "                        field_name = col.replace('outputs.', '')\n",
    "                        output_dict[field_name] = row[col]\n",
    "            \n",
    "            # Apply postprocessing\n",
    "            from src.utils.passport_processing import postprocess\n",
    "            processed = postprocess(output_dict)\n",
    "            \n",
    "            # Create new row with processed values\n",
    "            new_row = row.copy()\n",
    "            \n",
    "            # Update outputs with processed values\n",
    "            for key, value in processed.items():\n",
    "                col_name = f'outputs.{key}'\n",
    "                new_row[col_name] = value\n",
    "            \n",
    "            processed_rows.append(new_row)\n",
    "            \n",
    "            # Show progress\n",
    "            if (i + 1) % 10 == 0 or i == len(df) - 1:\n",
    "                print(f\"Processed {i + 1}/{len(df)} rows\", end='\\r')\n",
    "        \n",
    "        except Exception as e:\n",
    "            print(f\"\\nError processing row {i}: {e}\")\n",
    "            processed_rows.append(row)  # Keep original row on error\n",
    "    \n",
    "    print(\"\\nPostprocessing completed\")\n",
    "    \n",
    "    # Create new dataframe with processed data\n",
    "    processed_df = pd.DataFrame(processed_rows)\n",
    "    \n",
    "    # Update the output column with processed values\n",
    "    processed_df['output'] = processed_df.apply(\n",
    "        lambda row: json.dumps({key.split('.')[1]: row[key] for key in row.index if key.startswith(\"outputs.\")}), \n",
    "        axis=1\n",
    "    )\n",
    "    \n",
    "    # Save the processed results\n",
    "    results_dir = \"processed_results/\"\n",
    "    output_file = f\"{results_dir}{project_name}_processed_results.csv\"\n",
    "    processed_df.to_csv(output_file, index=False)\n",
    "    \n",
    "    return output_file, country\n",
    "\n",
    "def upload_to_sheets(output_file, res_agent, country):\n",
    "    \"\"\"Upload processed results to Google Sheets.\"\"\"\n",
    "    try:\n",
    "        res_agent.country = country\n",
    "        res_agent.upload_results(output_file)\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Error during upload: {e}\")\n",
    "        traceback.print_exc()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "res_agent = ResultsAgent()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Detected country/dataset: Kenya\n",
      "Read 135 rows from Kenya - gemini-2.5-pro - 347_results.csv\n",
      "Processed 135/135 rows\n",
      "Postprocessing completed\n",
      "142\n"
     ]
    }
   ],
   "source": [
    "file_path = \"results/Kenya - gemini-2.5-pro - 347_results.csv\"\n",
    "\n",
    "output_file, country = process_file(file_path)\n",
    "\n",
    "upload_to_sheets(output_file, res_agent, country)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "lang",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
